{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done by Mohamad Alkadri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common problem in Numismatics, the study of currency, is to detect whether two coins have been struck by the same die. Striking a coin refers to pressing an image into the blank metal disc of the coin. To produce a coin, one must use two dies: one for the observe side (head) and another for the reverse side (tail). Our goal in this problem is to automatically predict given two coins, each as a pair of images representing observe and reverse sides, 1)whether the observe sides of those two coins have been struck by the same die, and 2) whether the reverse sides of those two coins have been struck by the same die.\n",
    "\n",
    "To do this, we are providing you with a dataset consisting of 91 images of ancient coins of the Roman emperor Caracalla minted in Damascus. Each images depicts the observe and reverse sides of a coin. Also provided is an excel sheet that describes which observe sides and reverse sides of which coins have been struck by the same die. For example, given the following two rows from the excel sheet: \n",
    "\n",
    "1\tO1\tR1\n",
    "2\tO1\tR2\n",
    "\n",
    "This indicates that the observe sides of the two coins 1 and 2 have been struck by the same die, whereas their reverse sides were struck by different dies. \n",
    "\n",
    "You should build a deep learning model to solve the above problem. Once trained, your model should take two images representing two coins, and output whether their observe sides have been struck by the same die or not, and whether their reverse sides have been struck by the same die or not. You should submit your code as well as a write-up describing how you modeled the problem, what architecture and loss function you used, how did you split your data for training, validation, etc and finally some performance evaluation. Make sure you also submit your trained model so we can test it on other datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX2 Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I will solve the problem as following: <ul>\n",
    "    <li>1- Divide each image into two equal halfs, where the left side is the observe side and the right is the reverse side</li>\n",
    "    <li>2- I will use Siamese neural networks to solve this task.I will train the network on the coins so that it give small distances for the coins that are struck by the same die and big distances for coins that are struck by different dies</li>\n",
    "    <li>3- Finally, once the netwrok is trained well on the coin images, I will use one shot learning to tell if two halfs (images) are struck by the same die or not</li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>head</th>\n",
       "      <th>tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>O1</td>\n",
       "      <td>R1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>O1</td>\n",
       "      <td>R2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>O1</td>\n",
       "      <td>R3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>O1</td>\n",
       "      <td>R3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>O1</td>\n",
       "      <td>R4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>O1</td>\n",
       "      <td>R5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id head tail\n",
       "0   1   O1   R1\n",
       "1   2   O1   R2\n",
       "2   3   O1   R3\n",
       "3   4   O1   R3\n",
       "4   5   O1   R4\n",
       "5   6   O1   R5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"/home/cmps299/Desktop/ML/assi9/diestudy.csv\",header=None)\n",
    "df.columns=[\"id\",\"head\",\"tail\"]\n",
    "df[df[\"head\"]==\"O1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all Images and cut them in half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmps299/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/home/cmps299/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    }
   ],
   "source": [
    "# ''''' \n",
    "# I will do the following:\n",
    "# 1- read the image coin\n",
    "# 2- split the image into two halfs (head and coin)\n",
    "# 3- place all similar heads in one folder and all similar tails in one folder\n",
    "# ''''\n",
    "path=\"/home/cmps299/Desktop/ML/assi9/Die Study Images/\"\n",
    "path_output=\"/home/cmps299/Desktop/ML/assi9/data/\"\n",
    "\n",
    "# for each coin image in the folder ....\n",
    "for pic_path in glob.glob(path + \"*\"):\n",
    "    #extract image id from it's absolute path:\n",
    "    \n",
    "    index=pic_path.rfind('/') # index of last character (/)\n",
    "    pic_id = pic_path[index+1:] # to remove the absolute path \n",
    "    pic_id=int(re.sub('[^1-9]',\"\",pic_id)) # to remove .png .jpg ...\n",
    "    \n",
    "    # read the head and tail labels from diestudy.csv\n",
    "    head_label=df[df[\"id\"]==pic_id][\"head\"].values[0]\n",
    "    tail_label=df[df[\"id\"]==pic_id][\"tail\"].values[0]\n",
    "\n",
    "    #read the image \n",
    "    img =   img_to_array(load_img(pic_path))\n",
    "    height, width = img.shape[:-1]\n",
    "    \n",
    "    # Cut the image into two halfs (head and hail)\n",
    "    width_cutoff = width // 2\n",
    "    head = img[:, :width_cutoff,:]\n",
    "    tail = img[:, width_cutoff:,:]\n",
    "    \n",
    "    # Save each half\n",
    "    if not os.path.exists(path_output+head_label):\n",
    "        os.makedirs(path_output+head_label)\n",
    "        \n",
    "    if not os.path.exists(path_output+tail_label):\n",
    "        os.makedirs(path_output+tail_label)\n",
    "    \n",
    "    misc.imsave(path_output+head_label+\"/\"+str(pic_id)+\"head.png\", head)\n",
    "    misc.imsave(path_output+tail_label+\"/\"+str(pic_id)+\"tail.png\", tail)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply data Augmnetation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy import ndarray\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "from skimage import util\n",
    "\n",
    "def random_rotation(image_array: ndarray):\n",
    "    # pick a random degree of rotation between 25% on the left and 25% on the right\n",
    "    random_degree = random.uniform(-25, 25)\n",
    "    return sk.transform.rotate(image_array, random_degree)\n",
    "\n",
    "def random_noise(image_array: ndarray):\n",
    "    # add random noise to the image\n",
    "    return sk.util.random_noise(image_array)\n",
    "\n",
    "def horizontal_flip(image_array: ndarray):\n",
    "    # horizontal flip doesn't need skimage, it's easy as flipping the image array of pixels !\n",
    "    return image_array[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmps299/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  if sys.path[0] == '':\n",
      "/home/cmps299/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n",
      "/home/cmps299/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "path_output=\"/home/cmps299/Desktop/ML/assi9/data/\"\n",
    "unique_id=95\n",
    "# for each coin image in the folder ....\n",
    "for folder in glob.glob(path_output + \"*\"):\n",
    "    for pic in glob.glob(folder+\"/\" + \"*\"):\n",
    "        #apply transformation to the image(s)\n",
    "        img=cv2.imread(pic)\n",
    "        for i in range (4):\n",
    "            img1=random_rotation(img)\n",
    "            img2=random_noise(img)\n",
    "            img3=horizontal_flip(img)\n",
    "            misc.imsave(folder+\"/\"+str(unique_id)+\".png\", img1)\n",
    "            unique_id+=1\n",
    "            misc.imsave(folder+\"/\"+str(unique_id)+\".png\", img2)\n",
    "            unique_id+=1\n",
    "            misc.imsave(folder+\"/\"+str(unique_id)+\".png\", img3)\n",
    "            unique_id+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I took the implementation from this site: https://github.com/normandipalo/faceID_beta/blob/master/faceid_beta.ipynb \n",
    "However, with slight modification to fit my case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "'''os.mkdir(\"faceid_train\")\n",
    "os.mkdir(\"faceid_val\")'''\n",
    "import numpy\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img, img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define two functions: create_couple and create_wrong.\n",
    "#create_couple: creates two images that are struck by the same die\n",
    "# create_wrong: creates two images that are struck by different  die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "path=\"/home/cmps299/Desktop/ML/assi9/data/\"\n",
    "#Input preprocessing.\n",
    "#Here we create some functions that will create the input couple for our model, both correct and wrong couples. I created functions to have both depth-only input and RGBD inputs.\n",
    "import numpy as np\n",
    "import glob\n",
    "#import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def create_couple(file_path):\n",
    "    folder = np.random.choice(glob.glob(file_path + \"*\"))\n",
    "    while folder == \"datalab\":\n",
    "        folder = np.random.choice(glob.glob(file_path + \"*\"))\n",
    "        #  print(folder)\n",
    "    mat = np.zeros((480, 640), dtype='float32')\n",
    "    i = 0\n",
    "    j = 0\n",
    "    depth_file = np.random.choice(glob.glob(folder + \"/*.png\"))\n",
    "    mat = cv2.imread(depth_file)\n",
    "    mat=img_to_array(load_img(depth_file,target_size=(200, 200)))\n",
    "    mat = cv2.cvtColor(mat, cv2.COLOR_BGR2RGB)\n",
    "#     mat_small = mat[350:550, 550:750]\n",
    "    mat_small = mat\n",
    "#     plt.imshow(mat_small)\n",
    "#     plt.show()\n",
    "    \n",
    "    depth_file = np.random.choice(glob.glob(folder + \"/*.png\"))\n",
    "    mat2 = cv2.imread(depth_file)\n",
    "    mat2=img_to_array(load_img(depth_file,target_size=(200, 200)))\n",
    "\n",
    "    mat2 = cv2.cvtColor(mat2, cv2.COLOR_BGR2RGB)\n",
    "#     mat2_small = mat2[350:550, 550:750]\n",
    "    mat2_small = mat2\n",
    "#     plt.imshow(mat2_small)\n",
    "#     plt.show()\n",
    "\n",
    "    return np.array([mat_small, mat2_small])\n",
    "\n",
    "\n",
    "print(create_couple(path).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "def create_wrong(file_path):\n",
    "    folder = np.random.choice(glob.glob(file_path + \"*\"))\n",
    "    while folder == \"datalab\":\n",
    "        folder = np.random.choice(glob.glob(file_path + \"*\"))\n",
    "        \n",
    "        \n",
    "    depth_file = np.random.choice(glob.glob(folder + \"/*.png\"))\n",
    "    mat = cv2.imread(depth_file)\n",
    "    mat=img_to_array(load_img(depth_file,target_size=(200, 200)))\n",
    "\n",
    "    mat = cv2.cvtColor(mat, cv2.COLOR_BGR2RGB)\n",
    "#     mat_small = mat[350:550, 550:750]\n",
    "    mat_small=mat\n",
    "\n",
    "    folder2 = np.random.choice(glob.glob(file_path + \"*\"))\n",
    "    while folder == folder2 or folder2 == \"datalab\":  # it activates if it chose the same folder\n",
    "        folder2 = np.random.choice(glob.glob(file_path + \"*\"))\n",
    "        \n",
    "    depth_file = np.random.choice(glob.glob(folder2 + \"/*.png\"))\n",
    "    mat2 = cv2.imread(depth_file)\n",
    "    mat2=img_to_array(load_img(depth_file,target_size=(200, 200)))\n",
    "\n",
    "    mat2 = cv2.cvtColor(mat2, cv2.COLOR_BGR2RGB)\n",
    "    #     mat2_small = mat2[350:550, 550:750]\n",
    "    mat2_small=mat2\n",
    "    #plt.imshow(mat2_small)\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "    return np.array([mat_small, mat2_small])\n",
    "\n",
    "print(create_wrong(path).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network crafting.\n",
    "Now we create the network. We first manually create the *constrative loss*, then we define the network architecture starting from the SqueezeNet architecture, and then using it as a siamese-network for embedding faces into a manifold. (the network for now is very big and could be heavily optimized, but I just wanted to show a proof-of-concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Lambda, ELU, concatenate, GlobalAveragePooling2D, Input, BatchNormalization, SeparableConv2D, Subtract, concatenate\n",
    "from keras.activations import relu, softmax\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(inputs):\n",
    "    assert len(inputs) == 2, \\\n",
    "        'Euclidean distance needs 2 inputs, %d given' % len(inputs)\n",
    "    u, v = inputs\n",
    "    return K.sqrt(K.sum((K.square(u - v)), axis=1, keepdims=True))\n",
    "        \n",
    "\n",
    "def contrastive_loss(y_true,y_pred):\n",
    "    margin=1.\n",
    "    return K.mean((1. - y_true) * K.square(y_pred) + y_true * K.square(K.maximum(margin - y_pred, 0.)))\n",
    "   # return K.mean( K.square(y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire(x, squeeze=16, expand=64):\n",
    "    x = Convolution2D(squeeze, (1,1), padding='valid')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    left = Convolution2D(expand, (1,1), padding='valid')(x)\n",
    "    left = Activation('relu')(left)\n",
    "    \n",
    "    right = Convolution2D(expand, (3,3), padding='same')(x)\n",
    "    right = Activation('relu')(right)\n",
    "    \n",
    "    x = concatenate([left, right], axis=3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200, 200, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 98, 98, 64)   4864        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 98, 98, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 98, 98, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 48, 48, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 48, 48, 16)   1040        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 48, 16)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 48, 48, 16)   272         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 48, 48, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 48, 48, 16)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 48, 48, 16)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 48, 48, 32)   0           activation_3[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 48, 48, 16)   528         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 48, 48, 16)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 48, 48, 16)   272         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 48, 48, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 48, 48, 16)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 48, 48, 16)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 48, 48, 32)   0           activation_6[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 23, 23, 32)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 23, 23, 32)   1056        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 23, 23, 32)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 23, 23, 32)   1056        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 23, 23, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 23, 23, 32)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 23, 23, 32)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 23, 23, 64)   0           activation_9[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 23, 23, 32)   2080        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 23, 23, 32)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 23, 23, 32)   1056        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 23, 23, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 23, 23, 32)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 23, 23, 32)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 23, 23, 64)   0           activation_12[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 11, 11, 64)   0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 11, 11, 48)   3120        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 11, 11, 48)   0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 11, 11, 48)   2352        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 11, 11, 48)   20784       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 11, 11, 48)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 11, 11, 48)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 11, 11, 96)   0           activation_15[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 11, 11, 48)   4656        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 11, 11, 48)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 11, 11, 48)   2352        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 11, 11, 48)   20784       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 11, 11, 48)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 11, 11, 48)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 11, 11, 96)   0           activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 11, 11, 64)   6208        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 11, 11, 64)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 11, 11, 64)   4160        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 11, 11, 64)   36928       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 11, 11, 64)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 11, 11, 64)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 11, 11, 128)  0           activation_21[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 11, 11, 64)   8256        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 11, 11, 64)   0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 11, 11, 64)   4160        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 11, 11, 64)   36928       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 11, 11, 64)   0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 11, 11, 64)   0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 11, 11, 128)  0           activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 11, 11, 128)  0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 11, 11, 512)  66048       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 11, 11, 512)  0           conv2d_26[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 252,352\n",
      "Trainable params: 252,224\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_input=Input(shape=(200,200,3))\n",
    "\n",
    "x = Convolution2D(64, (5, 5), strides=(2, 2), padding='valid')(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "x = fire(x, squeeze=16, expand=16)\n",
    "\n",
    "x = fire(x, squeeze=16, expand=16)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "\n",
    "x = fire(x, squeeze=32, expand=32)\n",
    "\n",
    "x = fire(x, squeeze=32, expand=32)\n",
    "\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "\n",
    "x = fire(x, squeeze=48, expand=48)\n",
    "\n",
    "x = fire(x, squeeze=48, expand=48)\n",
    "\n",
    "x = fire(x, squeeze=64, expand=64)\n",
    "\n",
    "x = fire(x, squeeze=64, expand=64)\n",
    "\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Convolution2D(512, (1, 1), padding='same')(x)\n",
    "out = Activation('relu')(x)\n",
    "\n",
    "\n",
    "modelsqueeze= Model(img_input, out)\n",
    "\n",
    "modelsqueeze.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 200, 200, 3)       0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 11, 11, 512)       252352    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 61952)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               31719936  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 32,037,952\n",
      "Trainable params: 32,037,824\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 200, 200, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 200, 200, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 128)          32037952    input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           model_2[1][0]                    \n",
      "                                                                 model_2[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 32,037,952\n",
      "Trainable params: 32,037,824\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "im_in = Input(shape=(200,200,3))\n",
    "#wrong = Input(shape=(200,200,3))\n",
    "\n",
    "x1 = modelsqueeze(im_in)\n",
    "#x = Convolution2D(64, (5, 5), padding='valid', strides =(2,2))(x)\n",
    "\n",
    "#x1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x1)\n",
    "\n",
    "\"\"\"\n",
    "x1 = Convolution2D(256, (3,3), padding='valid', activation=\"relu\")(x1)\n",
    "x1 = Dropout(0.4)(x1)\n",
    "\n",
    "x1 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1))(x1)\n",
    "\n",
    "x1 = Convolution2D(256, (3,3), padding='valid', activation=\"relu\")(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dropout(0.4)(x1)\n",
    "\n",
    "x1 = Convolution2D(64, (1,1), padding='same', activation=\"relu\")(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dropout(0.4)(x1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "x1 = Flatten()(x1)\n",
    "\n",
    "x1 = Dense(512, activation=\"relu\")(x1)\n",
    "x1 = Dropout(0.2)(x1)\n",
    "#x1 = BatchNormalization()(x1)\n",
    "feat_x = Dense(128, activation=\"linear\")(x1)\n",
    "feat_x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(feat_x)\n",
    "\n",
    "\n",
    "model_top = Model(inputs = [im_in], outputs = feat_x)\n",
    "\n",
    "model_top.summary()\n",
    "\n",
    "im_in1 = Input(shape=(200,200,3))\n",
    "im_in2 = Input(shape=(200,200,3))\n",
    "\n",
    "feat_x1 = model_top(im_in1)\n",
    "feat_x2 = model_top(im_in2)\n",
    "\n",
    "\n",
    "lambda_merge = Lambda(euclidean_distance)([feat_x1, feat_x2])\n",
    "\n",
    "\n",
    "model_final = Model(inputs = [im_in1, im_in2], outputs = lambda_merge)\n",
    "\n",
    "model_final.summary()\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "\n",
    "sgd = SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "model_final.compile(optimizer=adam, loss=contrastive_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning phase.\n",
    "We write the generators that will give our model batches of data to train on, then we run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batch_size):\n",
    "  \n",
    "  while 1:\n",
    "    X=[]\n",
    "    y=[]\n",
    "    switch=True\n",
    "    for _ in range(batch_size):\n",
    "   #   switch += 1\n",
    "      if switch:\n",
    "     #   print(\"correct\")\n",
    "        X.append(create_couple(path).reshape((2,200,200,3)))\n",
    "        y.append(np.array([0.]))\n",
    "      else:\n",
    "     #   print(\"wrong\")\n",
    "        X.append(create_wrong(path).reshape((2,200,200,3)))\n",
    "        y.append(np.array([1.]))\n",
    "      switch=not switch\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    XX1=X[0,:]\n",
    "    XX2=X[1,:]\n",
    "    yield [X[:,0],X[:,1]],y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_generator(batch_size):\n",
    "  \n",
    "  while 1:\n",
    "    X=[]\n",
    "    y=[]\n",
    "    switch=True\n",
    "    for _ in range(batch_size):\n",
    "      if switch:\n",
    "        X.append(create_couple(path).reshape((2,200,200,3)))\n",
    "        y.append(np.array([0.]))\n",
    "      else:\n",
    "        X.append(create_wrong(path).reshape((2,200,200,3)))\n",
    "        y.append(np.array([1.]))\n",
    "      switch=not switch\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    XX1=X[0,:]\n",
    "    XX2=X[1,:]\n",
    "    yield [X[:,0],X[:,1]],y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generator(16)\n",
    "val_gen = val_generator(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/42\n",
      "30/30 [==============================] - 38s 1s/step - loss: 0.2587 - val_loss: 0.4971\n",
      "Epoch 2/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.2506 - val_loss: 0.4966\n",
      "Epoch 3/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.2546 - val_loss: 0.4971\n",
      "Epoch 4/42\n",
      "30/30 [==============================] - 33s 1s/step - loss: 0.2527 - val_loss: 0.4959\n",
      "Epoch 5/42\n",
      "30/30 [==============================] - 33s 1s/step - loss: 0.2518 - val_loss: 0.4954\n",
      "Epoch 6/42\n",
      "30/30 [==============================] - 35s 1s/step - loss: 0.2550 - val_loss: 0.4938\n",
      "Epoch 7/42\n",
      "30/30 [==============================] - 33s 1s/step - loss: 0.2524 - val_loss: 0.4937\n",
      "Epoch 8/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.2521 - val_loss: 0.4897\n",
      "Epoch 9/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.2512 - val_loss: 0.4916\n",
      "Epoch 10/42\n",
      "30/30 [==============================] - 35s 1s/step - loss: 0.2569 - val_loss: 0.4903\n",
      "Epoch 11/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.2531 - val_loss: 0.4904\n",
      "Epoch 12/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.2522 - val_loss: 0.4846\n",
      "Epoch 13/42\n",
      "30/30 [==============================] - 35s 1s/step - loss: 0.2555 - val_loss: 0.4746\n",
      "Epoch 14/42\n",
      "30/30 [==============================] - 35s 1s/step - loss: 0.2415 - val_loss: 0.3294\n",
      "Epoch 15/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1930 - val_loss: 0.2842\n",
      "Epoch 16/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.2123 - val_loss: 0.3217\n",
      "Epoch 17/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1924 - val_loss: 0.2626\n",
      "Epoch 18/42\n",
      "30/30 [==============================] - 33s 1s/step - loss: 0.1808 - val_loss: 0.2461\n",
      "Epoch 19/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1759 - val_loss: 0.1860\n",
      "Epoch 20/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1720 - val_loss: 0.1856\n",
      "Epoch 21/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1545 - val_loss: 0.1629\n",
      "Epoch 22/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1472 - val_loss: 0.1800\n",
      "Epoch 23/42\n",
      "30/30 [==============================] - 35s 1s/step - loss: 0.1547 - val_loss: 0.1496\n",
      "Epoch 24/42\n",
      "30/30 [==============================] - 35s 1s/step - loss: 0.1503 - val_loss: 0.1414\n",
      "Epoch 25/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1596 - val_loss: 0.1314\n",
      "Epoch 26/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1276 - val_loss: 0.1024\n",
      "Epoch 27/42\n",
      "30/30 [==============================] - 33s 1s/step - loss: 0.1278 - val_loss: 0.1316\n",
      "Epoch 28/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1276 - val_loss: 0.1302\n",
      "Epoch 29/42\n",
      "30/30 [==============================] - 35s 1s/step - loss: 0.1327 - val_loss: 0.1300\n",
      "Epoch 30/42\n",
      "30/30 [==============================] - 36s 1s/step - loss: 0.1548 - val_loss: 0.1575\n",
      "Epoch 31/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1314 - val_loss: 0.1613\n",
      "Epoch 32/42\n",
      "30/30 [==============================] - 35s 1s/step - loss: 0.1282 - val_loss: 0.1053\n",
      "Epoch 33/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1281 - val_loss: 0.1177\n",
      "Epoch 34/42\n",
      "30/30 [==============================] - 36s 1s/step - loss: 0.1185 - val_loss: 0.1300\n",
      "Epoch 35/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1312 - val_loss: 0.0887\n",
      "Epoch 36/42\n",
      "30/30 [==============================] - 33s 1s/step - loss: 0.1127 - val_loss: 0.1226\n",
      "Epoch 37/42\n",
      "30/30 [==============================] - 33s 1s/step - loss: 0.1419 - val_loss: 0.1308\n",
      "Epoch 38/42\n",
      "30/30 [==============================] - 33s 1s/step - loss: 0.1113 - val_loss: 0.0994\n",
      "Epoch 39/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1384 - val_loss: 0.0993\n",
      "Epoch 40/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1241 - val_loss: 0.0773\n",
      "Epoch 41/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1077 - val_loss: 0.0662\n",
      "Epoch 42/42\n",
      "30/30 [==============================] - 34s 1s/step - loss: 0.1152 - val_loss: 0.0702\n"
     ]
    }
   ],
   "source": [
    "outputs = model_final.fit_generator(gen, steps_per_epoch=30, epochs=42, validation_data = val_gen, validation_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save  model's architecture and weights for a future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "output_source=\"/media/cmps299/Seagate Expansion Drive/model/\"\n",
    "# serialize model to JSON\n",
    "\n",
    "# model_json = model_final.to_json()\n",
    "# with open(output_source+\"model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model_final.save_weights(output_source+\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# later...\n",
    "# # load json and create model\n",
    "# # json_file = open(output_source+'model.json', 'r')\n",
    "# # loaded_model_json = json_file.read()\n",
    "# # json_file.close()\n",
    "# # model_final = model_from_json(loaded_model_json)\n",
    "\n",
    "# # load weights into new model\n",
    "# model_final.load_weights(output_source+\"model.h5\")\n",
    "# print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00881088338792324"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check distance between two coins that are struck by the same die \n",
    "cop = create_couple(path)\n",
    "\n",
    "model_final.evaluate([cop[0].reshape((1,200,200,3)), cop[1].reshape((1,200,200,3))], np.array([0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24336275458335876"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check distance between two coins that are struck by different  dies \n",
    "cop = create_wrong(path)\n",
    "\n",
    "model_final.evaluate([cop[0].reshape((1,200,200,3)), cop[1].reshape((1,200,200,3))], np.array([0.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice the model's performance is good, because the model is giving small distance for coins that are struck by the same die and big distnace otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to find a threshold for the distance, so that any distance below the threshold means that the coins are stuck by the same die and vice versa. <br>\n",
    "Through random testing, I found out a threshould of (0.4) is a good threshold. However, a beter way to choose it, is by taking the mean distance between all coins that are struk by the same die, and the mean for the coins that are struck by different dies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0854896761525265 0.8982109160791151\n"
     ]
    }
   ],
   "source": [
    "# do random testing to find a good threshold\n",
    "similar_dis=0\n",
    "diff_dis=0\n",
    "for i in range (200):\n",
    "    cop = create_couple(path)\n",
    "    similar_dis+=model_final.evaluate([cop[0].reshape((1,200,200,3)), cop[1].reshape((1,200,200,3))], np.array([0.]),verbose=0)\n",
    "    \n",
    "    cop = create_wrong(path)\n",
    "    diff_dis+=model_final.evaluate([cop[0].reshape((1,200,200,3)), cop[1].reshape((1,200,200,3))], np.array([0.]),verbose=0)\n",
    "    \n",
    "print(similar_dis/200,diff_dis/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.4 # is a good threeshold. We can improve the model performance by choosing a better threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a code that takes two images and tell whether their heads and tails are struck by the same die or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head sides of the two coins 1 and 2 have been struck by the same die. Predicted: True\n",
      "Tail sides of the two coins 1 and 2 have been struck by different dies. Predicted: False\n",
      "Actual results for heads and tails:  True False Respectively\n",
      "heads accuracy:  1.0\n",
      "tails accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "coin1_path=\"/home/cmps299/Desktop/ML/assi9/Die Study Images/1.jpg\"\n",
    "coin2_path=\"/home/cmps299/Desktop/ML/assi9/Die Study Images/2.jpg\"\n",
    "\n",
    "head_total_right_hits=0\n",
    "tail_total_righ_hits=0\n",
    "n_sample=0\n",
    "# n_sample=0\n",
    "def predict_model(coin1_path,coin2_path,check_acc=False):\n",
    "    global head_total_right_hits,tail_total_righ_hits,n_sample\n",
    "    n_sample+=1\n",
    "    # Read the image\n",
    "    def split_image(path):\n",
    "\n",
    "        img=img_to_array(load_img(path,target_size=(200, 400,3)))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        height, width = img.shape[:-1]\n",
    "\n",
    "        # Cut the image in half\n",
    "        width_cutoff = width // 2\n",
    "        head = img[:, :width_cutoff,:]\n",
    "        tail = img[:, width_cutoff:,:]\n",
    "\n",
    "        return head,tail\n",
    "\n",
    "    img1_head,img1_tail=split_image(coin1_path)\n",
    "    img2_head,img2_tail=split_image(coin2_path)\n",
    "    \n",
    "    #extract img1 and img2 label\n",
    "    index=coin1_path.rfind('/') # index of last character (/)\n",
    "    pic_id = coin1_path[index+1:] # to remove the absolute path \n",
    "    img1_id=int(re.sub('[^1-9]',\"\",pic_id)) # to remove .png .jpg ...\n",
    "    \n",
    "    index=coin2_path.rfind('/') # index of last character (/)\n",
    "    pic_id = coin2_path[index+1:] # to remove the absolute path \n",
    "    img2_id=int(re.sub('[^1-9]',\"\",pic_id)) # to remove .png .jpg ...\n",
    "    \n",
    "    # read the head and tail labels from diestudy.csv\n",
    "    img1_head_label=df[df[\"id\"]==img1_id][\"head\"].values[0]\n",
    "    img1_tail_label=df[df[\"id\"]==img1_id][\"tail\"].values[0]\n",
    "    img2_head_label=df[df[\"id\"]==img2_id][\"head\"].values[0]\n",
    "    img2_tail_label=df[df[\"id\"]==img2_id][\"tail\"].values[0]\n",
    "\n",
    "    head_distance=model_final.evaluate([img1_head.reshape((1,200,200,3)), img2_head.reshape((1,200,200,3))], np.array([0.]),verbose=0)\n",
    "    tail_distance=model_final.evaluate([img1_tail.reshape((1,200,200,3)), img2_tail.reshape((1,200,200,3))], np.array([0.]),verbose=0)\n",
    "    \n",
    "    if not check_acc:\n",
    "        if head_distance < threshold:\n",
    "            print(\"Head sides of the two coins 1 and 2 have been struck by the same die. Predicted:\",True)\n",
    "        else:\n",
    "            print(\"Head sides of the two coins 1 and 2 have been struck by different dies. Predicted:\",False)\n",
    "        if tail_distance < threshold:\n",
    "            print(\"Tail sides of the two coins 1 and 2 have been struck by the same die. Predicted:\",True)\n",
    "        else:\n",
    "            print(\"Tail sides of the two coins 1 and 2 have been struck by different dies. Predicted:\",False)\n",
    "        \n",
    "        print(\"Actual results for heads and tails: \",img1_head_label==img2_head_label,img1_tail_label==img2_tail_label,\"Respectively\")\n",
    "        \n",
    "        if img1_head_label==img2_head_label and head_distance < threshold or (img1_head_label!=img2_head_label and head_distance > threshold) :\n",
    "            head_total_right_hits+=1\n",
    "        if img1_tail_label==img2_tail_label and tail_distance < threshold or (img1_tail_label!=img2_tail_label and tail_distance > threshold):\n",
    "            tail_total_righ_hits+=1\n",
    "\n",
    "\n",
    "predict_model(coin1_path,coin2_path)\n",
    "print(\"heads accuracy: \",head_total_right_hits/(n_sample))\n",
    "print(\"tails accuracy: \",tail_total_righ_hits/(n_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/88.bmp\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/32.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/62.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/22.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/68.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/33.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/20.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/78.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/17.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/48.JPG\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/67.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/74.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/73.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/49.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/10.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/58.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/30.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/85.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/55.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/54.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/1.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/29.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/82.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/63.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/75.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/8.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/59.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/3.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/16.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/4.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/41.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/81.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/47.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/36.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/34.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/61.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/56.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/11.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/91.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/71.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/40.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/25.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/72.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/18.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/9.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/42.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/45.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/31.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/23.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/60.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/19.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/24.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/37.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/84.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/35.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/57.bmp\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/38.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/80.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/86.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/2.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/6.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/14.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/90.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/52.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/7.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/77.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/53.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/87.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/15.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/46.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/12.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/89.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/79.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/39.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/64.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/50.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/83.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/28.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/44.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/13.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/76.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/26.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/43.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/51.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/69.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/70.bmp\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/21.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/66.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/27.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/5.jpg\n",
      "/home/cmps299/Desktop/ML/assi9/Die Study Images/65.jpg\n",
      "heads accuracy:  0.5516483516483517\n",
      "tails accuracy:  0.8105006105006105\n"
     ]
    }
   ],
   "source": [
    "# we have 91 images. do the predictions on them\n",
    "dir_path='/home/cmps299/Desktop/ML/assi9/Die Study Images'\n",
    "path='/home/cmps299/Desktop/ML/assi9/Die Study Images/'\n",
    "head_total_right_hits=0\n",
    "tail_total_righ_hits=0\n",
    "n_sample=0\n",
    "i=0\n",
    "for pic1 in glob.glob(path + \"*\"):\n",
    "    j=0\n",
    "    print(pic1)\n",
    "    for pic2 in glob.glob(path + \"*\"):\n",
    "        if j!=i: # so that we don't rea the same image twice \n",
    "            predict_model(pic1,pic2) \n",
    "        j+=1\n",
    "    i+=1\n",
    "    # (i) and (j) to prevent duplicate image\n",
    "print(\"heads accuracy: \",head_total_right_hits/(n_sample))\n",
    "print(\"tails accuracy: \",tail_total_righ_hits/(n_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heads accuracy:  0.5516483516483517\n",
      "tails accuracy:  0.8105006105006105\n"
     ]
    }
   ],
   "source": [
    "print(\"heads accuracy: \",head_total_right_hits/(n_sample))\n",
    "print(\"tails accuracy: \",tail_total_righ_hits/(n_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heads accuracy:  0.5516483516483517 (55% of the predicted heads are predicted correct) <br>\n",
    "tails accuracy:  0.8105006105006105 (81% of the predicted tails are predicted correct) <br>\n",
    "The performance can be easily enhanced by picking a better threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the jupyter notebook to Assignment8_*netid*.ipynb (Assignment8_xyz01.ipynb) and upload it on Moodle no later than Wednesday, Nov 28 11:55 pm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
